\section{Анализ предметной области}

\subsection{Распознавание объектов на изображениях}

\subsubsection{Исторический обзор}
Задачи распознавания объектов на изображениях являются основными в области компьютерного зрения.
Разработанные методы могут применяться в различных сферах, начиная от идентификации пользователя по фотографии, 
заканчивая сложными системами видеонаблюдения и робототехникой. В целом, все существующие методы можно разделить на 
две категории: методы ручного выделения признаков (handcrafted features) и методы, основанные на нейронных сетях.
При использовании методов ручного выделения, какие-то признаки изображений выделяются по заранее определенному алгоритму 
с учетом специфики данных. Примером таких признаков могут служить границы объектов. Так как с учетом специфики изображений 
мы знаем, что если значения соседних пикселей сильно различаются, то это может свидетельствовать о том, что в этом
месте проходит граница между двумя объектами. В свою очередь, методы с применением нейронных сетей устроены по другому. 
Нейронная сеть сама определяет набор признаков на основе данных, полученных в процессе обучения.

До 2012 года наибольшей точностью обладали методы с ручным выделением признаков. Тогда пользовались
популярностью методы основанные на гистограммах направленных градиентов и признаках Хаара. Однако в 2012 году эта 
тенденция изменилась после того, как на соревновании ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 
методы с применением искусственных нейронных сетей показали лучшие результаты.

Участникам соревнования ILSVRC предлагается разработать алгоритмы для задач классификации изображений и обнаружения
объектов \cite{ILSVRC}. Сами соревнования начали проводиться с 2010 года и использовали ImageNet, в качестве базы данных 
изображений. ImageNet - это база данных, которая содержит более 14 миллионов изображений, объекты на которых могут 
относиться к одному из 1000 классов \cite{IMAGE_NET}. 

На рисунке \ref{imagenet_results} представлена диаграмма с лучшими решениями задачи классификации на ILSVRC с 2011 
по 2016 год. По горизонтальной оси откладывается время, по вертикальной - процент ошибок. Для каждого из годов в скобках
указано название предложенного метода. Как можно заметить, в 2012 году произошел достаточно резкий скачок в точности 
классификации изображений, почти на 10\%. Этот прирост обусловлен тем, что авторы решили отойти от классических подходов 
и разработали метод, в основе которого лежали сверточные нейронные сети (СНС). В последующие года победителями также 
становились методы, основанные на СНС, а в 2015 году предложенное решение стало классифицировать изображения даже 
точнее, чем это может выполнить человек.

\addimg{imagenet_results}{0.8}{Лучшие решения ILSVRC с 2011 по 2016}{imagenet_results}

\subsubsection{Концепция сверточных нейронных сетей}
Идея сверточных нейронных сетей (СНС) появилась достаточно давно, примерно в начале 80-х годов \cite{DIVE_INTO_DL}.
По принципу своей работы СНС схожи с тем, как функционирует зрительная кора головного мозга человека. Именно из-за этого
сходства СНС удается достигать наилучших результатов в различных задачах компьютерного зрения.

СНС получили свое название из-за операции свертки (convolution), которая как раз и выделяет интересующие признаки
на изображениях. При операциях свертки используется так называемые фильтры (filters) или ядра (kernels). В случаях 
двумерной свертки это двумерная матрица весов нечетного размера (обычно $3 \times 3$ или $5 \times 5$). Операцию 
двумерной свертки для изображения заданного в оттенках серого (то есть каждый пиксель занимает один байт) 
можно описать следующим образом:

\begin{enumerate}
\item переместиться в левый-верхний угол входного изображения, таким образом, чтобы каждому весу фильтра соответствовал
пиксель изображения;
\item выполнить произведение соответствующих пикселей изображения и весов фильтра;
\item просуммировать полученные произведения. Эта сумма и будет результатом свертки области изображения под фильтром;
\item сдвинуть фильтр вправо на величину шага (stride). При достижении правой границы изображения спуститься
на строку ниже;
\item вернуться на шаг два, пока полностью не будет обработано все изображение.
\end{enumerate}

Как можно понять из приведенного описания, операция свертки уменьшает размер результирующего изображения, то есть происходит
потеря информации на краях изображения. Для решения этой проблемы стали использовать дополнение (padding). Чаще всего применяется
нулевое дополнение, когда изображение по краям дополняется нулями. Пример нулевого дополнения для картинки размером 
$7 \times 7$ и размером фильтра $5 \times 5$ представлен на рисунке \ref{zero_padding}.

В настоящее время большинство изображений используют цветовую модель RGB, когда каждый пиксель кодируется тремя числами 
-- интенсивностью соответственно красного, зеленого и синего цветов. Поэтому фильтрам в операции свертки добавляют
третью компоненту, чтобы покрывались все каналы входного изображения.

\addimg{zero_padding}{0.8}{Пример нулевого дополнения}{zero_padding}

Основными составляющими СНС являются сверточные слои, где обычно происходит порядка 90-95\% всех вычислительных операций \cite{PARALLEL_CNN}.
Каждый из слоев может содержать в себе некоторое количество фильтров, размер и количество которых определяются архитектурой СНС.
После применения фильтра к результату предыдущего слоя формируется так называемая карта признаков (feature map). То есть
каждый фильтр в сверточном слое выделяет только те признаки, которые он выучил в процессе обучения. В итоге фильтры первого 
сверточного слоя будут выделять какие-то простые признаки (прямые, наклонные), далее фильтры второго слоя смогут выделять 
уже полноценные фигуры (квадраты, треугольники) и чем глубже будет находиться слой, тем все более сложные конструкции он 
сможет выделять.

Так как для всей операции свертки используются одинаковые веса фильтров, то это приводит к более быстрому обучению 
сверточных слоев по сравнению с теми же полносвязными слоями. Как раз идея разделения весов фильтров является уместной
для задач обработки изображений, так как основным свойством всех изображений является их пространственная инвариантность 
\cite{NN_AND_DL}. Иными словами любая форма на изображении должна обрабатываться аналогичным образом, в не зависимости 
от ее расположения.

За сверточным слоем в СНС следует операция активации. Основной задачей функции активации в нейронных сетях является 
добавление нелинейности, иначе при отсутствии функции активации (либо при использовании линейной функции активации) 
увеличение количества слоев не давало бы никакого выигрыша \cite{NN_AND_DL}. Иначе говоря при использовании функции 
активации нейронная сеть может выучить более сложные зависимости во входных данных. 

Классическими примерами функций активации являются сигмоида и гиперболический тангенс. На рисунке \ref{activation_funcs}
приведены графики данных функций активации. Однако в последнее время все чаще используется выпрямленная линейная 
функция активации (ReLU). Причин у этого несколько:

\addimg{activation_funcs}{0.6}{Примеры графиков функций активации}{activation_funcs}

\begin{enumerate}
\item значения функций сигмоиды и гиперболического тангенса изменяются незначительно при стремлении аргумента к 
$+\infty$ или $-\infty$. Это становится причиной проблемы затухающего градиента (vanishing gradient), что приводит к 
большему времени обучения для более глубоких слоев. Функция активации ReLU лишена этой проблемы. Именно благодаря этой 
особенности авторам известной СНС AlexNet удалось в шесть раз ускорить процесс обучения при использовании ReLU по сравнению 
с гиперболическим тангенсом \cite{ALEXNET};
\item расчет значений функций сигмоиды и гиперболического тангенса более ресурсоемкий с точки зрения центрального процессора,
так как необходимо выполнять операции возведения в степень и деления.
\end{enumerate}

После применения функции активации чаще следует операция субдискретизации (pooling). Существует несколько типов 
данной операции, но на данный момент широко используемой является операция взятия максимума (max-pooling). Сама операция
состоит из разбиения результатов работы предыдущего слоя на локальные области размером $w \times h$ и взятия максимума 
из этой области.

Основной целью операции субдискретизации является уменьшение размеров карты признаков. Это приводит к тому, что последующие
операции свертки будут оперировать над все большей областью исходного изображения. Также очевидным следствием данной 
операции будет уменьшение количества вычислений в последующих слоях. Помимо этого, в результате операции субдискретизации
СНС становится более устойчивой к небольшим трансформациям изображения вроде сдвига или поворота \cite{DIVE_INTO_DL}.

После чередования нескольких сверточных слоев с операциями активации и субдискретизации, завершающим обычно служит
полносвязный (fully connected) слой, который уже непосредственно соединяется с выходным слоем СНС. В действительности количество
полносвязных слоев может быть больше одного, этот вопрос остается на усмотрение авторам СНС. Главной задачей полносвязного
слоя является установления соответствия, какие из высокоуровневых признаков принадлежат к тому или иному классу. Чаще всего 
полносвязные слои из-за своей структуры содержат порядка 95\% всех параметров СНС, но при этом в этих слоях происходит 
порядка 5-10\% от всех вычислительных операций \cite{PARALLEL_CNN}.

Обобщая все вышесказанное, типичная архитектура СНС состоит из чередующихся слоев свертки с операциями активации и 
субдискретизации и завершающим это полносвязным слоем. Структура выходного слоя определяется задачей, решаемой СНС. 
Например, для задач классификации размер выходного слоя будет равен количеству классов. На рисунке \ref{alexnet}
представлена архитектура известной СНС AlexNet без частей, связанных с разделением слоев между двумя GPU. Современные
СНС могут содержать десятки сверточных слоев, поэтому часто операции активации даже не приводят на рисунках, подразумевая,
что после каждого сверточного слоя следует ReLU активация. Операция субдискретизации обозначается как MP (сокращение от 
max-pooling) и можно заметить, что операция субдискретизации может отсутствовать на выходе некоторых сверточных слоев.

\addimg{alexnet}{0.8}{Архитектура AlexNet без GPU-разделения \cite{NN_AND_DL}}{alexnet}

\subsubsection{Методы для распознавания объектов}
% ToDo: add info about calculating mAP

После большого успеха СНС в решении задач классификации, данный тип нейронных сетей также начал широко применяться 
в методах для распознавания объектов. Под распознаванием объектов будем понимать обнаружение объектов на изображениях, 
то есть определение координат ограничивающей рамки (bounding box), и определение их класса. 

На данный момент существующие методы для распознавания объектов можно разделить на два класса: одношаговые (one-stage) 
и двухшаговые (two-stage). В двухшаговых методах входное изображение разбивается на регионы (regions of interest, RoI) 
и далее каждый из RoI обрабатывается отдельно. Также определением координат ограничивающих рамок и классов объектов 
занимаются разные компоненты. При использовании одношаговых методов входное изображение полностью обрабатывается нейронной 
сетью, которая одновременно предсказывает, и координаты ограничивающих рамок, и классы объектов. В общем случае 
двухшаговые методы требуют на обработку больше времени по сравнению с одношаговыми, но при этом обладают 
большей точностью распознавания \cite{DL_IN_OBJECT_DETECTION}.

\textbf{R-CNN}

Первым представителем двухшаговых методов стал метод Regions with CNN features (R-CNN). Тогда авторам удалось повысить 
среднюю точность (mean average precision) распознавания объектов на 30\% по сравнению с предыдущими методами на датасете 
PASCAL VOC2012 \cite{RCNN}. На рисунке \ref{rcnn} представлено описание метода R-CNN. Рассмотрим каждый из шагов отдельно:

\addimg{rcnn}{0.8}{Описание метода R-CNN \cite{RCNN}}{rcnn}

\begin{enumerate}
\item изначально из входного изображения выделяются примерно 2000 RoI, на которых могут находиться объекты. Для 
поиска RoI авторы использовали метод селективного поиска (selective search). Упрощая, можно сказать, что метод 
селективного поиска определяет границы объектов по изменению цветов и интенсивностей соседних пикселей;
\item так как выделенные RoI могут быть разных размеров, то каждый из них необходимо приводить к входным размерам 
СНС -- $227 \times 227$;
\item после каждый из RoI передается на вход СНС. В качестве СНС авторы используют архитектуру AlexNet, у которой 
удален последний полносвязный слой. В связи с этим на выходе получается вектор признаков размерностью 4096;
\item сама классификация объектов осуществлялась с помощью метода опорных векторов (support-vector machine, SVM). Для 
каждого из классов был свой бинарный SVM-классификатор;
\item координаты объектов, полученные на первом шаге могут быть неточными, поэтому каждый из RoI в конце еще обрабатывается 
моделью линейной регрессии, отдельной для каждого из классов. По утверждениям авторов данный шаг увеличил среднюю точность
на 3-4\% \cite{RCNN}.
\end{enumerate}

\textbf{Fast R-CNN}

Метод R-CNN обладал хорошей точностью, однако общее время обработки изображения было крайне высоким. Например, при использовании
в качестве СНС архитектуру VGG16 обработка одного изображения методом R-CNN занимала 47 секунд (измерения проводились при 
использовании одной GPU NVIDIA Tesla K40) \cite{FAST_RCNN}. У этого было несколько причин. Во-первых, проход 2000 RoI через
СНС требовал много вычислительных и временных ресурсов, особенно ситуация ухудшалась для более глубоких СНС, таких как, VGG16.
Во-вторых, модель линейной регрессии зависела от результатов классификации, что не давало выполнять эти операции параллельно.
Также наличие 3 отдельных моделей требовало много времени и дискового пространства в процессе обучения \cite{FAST_RCNN}.

Для решения вышеперечисленных проблем авторами R-CNN вскоре был представлен улучшенный метод -- Fast R-CNN. На рисунке \ref{fast_rcnn}
приведено описание данного метода. Рассмотрим обновленный метод подробнее:

\addimg{fast_rcnn}{0.8}{Описание метода Fast R-CNN \cite{FAST_RCNN}}{fast_rcnn}

\begin{enumerate}
\item теперь входное изображение сразу поступает на вход СНС. Параллельно с этим из входного изображения выделяются RoI 
с помощью того же метода селективного поиска;
\item далее из карты признаков выделяется вектор признаков для каждого из RoI. Решением этой задачи 
занимается RoI pooling layer;
\item после вектор признаков передается в полносвязный слой, а затем следуют два параллельных слоя - Softmax и линейная регрессия.
Слой Softmax определяет класс объекта, а слой линейной регрессии уточняет координаты ограничивающей рамки.
\end{enumerate}

\textbf{Faster R-CNN}

Метод Fast R-CNN устранил основные проблемы первоначального метода R-CNN, но все равно еще плохо подходил для тех же задач 
обработки видео. Например, на обработку одного изображения уходило порядка двух секунд и большая часть этого времени тратилась 
на поиск RoI с помощью селективного поиска \cite{FASTER_RCNN}. Для решения этой проблемы авторы метода Faster R-CNN 
заменили селективный поиск отдельной СНС, которую назвали Region Proposal Network (RPN). Все остальные компоненты 
метода Fast R-CNN остались без изменений.

На рисунке \ref{faster_rcnn} иллюстрируется принцип работы RPN. На вход RPN передается карта признаков, ранее выделенных
с помощью СНС. Далее по карте признаков проходятся "скользящим окном" (sliding window) размером $3 \times 3$. Для каждого
положения окна выделяется \emph{k} RoI, где \emph{k} это количество якорей (anchors). Якоря используются далее для определения 
абсолютных координат RoI. После наложения якорей на текущее положение скользящего окна, из карты признаков выделяется
вектор признаков размерностью 256. Затем полученный вектор передается в два параллельных сверточных слоя с размером фильтра
$1 \times 1$. Слой \emph{cls} имеет \emph{2k} выходов для определения вероятности наличия/отсутствия объекта внутри RoI.
В свою очередь слой \emph{reg} имеет \emph{4k} выходов, каждый из которых соответствует поправкам к координатам якорей.

Проведенные авторами тесты показывают, что предложенная ими RPN тратит порядка 10 мс на выделение RoI \cite{FASTER_RCNN}. 
Благодаря этому удалось достичь скорости обработки порядка пяти кадров в секунду, что соответствует 200 мс на кадр. При всем 
при этом предложенный метод оказался более точным. На датасете PASCAL VOC2007 прирост средней точности составил 
порядка 9\% по сравнению с методом Fast R-CNN.

\addimg{faster_rcnn}{0.7}{Принцип работы RPN \cite{FASTER_RCNN}}{faster_rcnn}

\textbf{YOLO}

Метод You Only Look Once (YOLO) является одним из первых представителей одношаговых методов. По утверждению авторов 
последняя версия метода YOLOv4 обладает лучшей эффективностью среди существующих методов \cite{YOLO4}. Под эффективностью
понимается отношение точности к времени обработки одного изображения. На текущий момент представлено четыре версии метода: 
YOLO \cite{YOLO}, YOLOv2 \cite{YOLO2}, YOLOv3 \cite{YOLO3} и YOLOv4 \cite{YOLO4}. Рассмотрим обобщенно принцип работы 
данных методов.

Изначально авторы первой версии YOLO решили рассмотреть проблему распознавания объектов как проблему регрессии, то есть
одновременное предсказание координат ограничивающей рамки и класса объекта внутри нее. Сам метод разделяет входное изображение
на сетку размером $S \times S$. Каждому из объектов присваивается клетка (grid cell) из этой сетки. Если объект занимает 
более одной клетки, то ему присваивается та клетка, внутри которой расположен центр этого объекта. Каждая клетка предсказывает 
\emph{B} ограничивающих рамок, разных форм и размеров. 

Само предсказание для ограничивающих рамок состоит из пяти чисел: $(x, y, w, h, a)$. Числа $(x, y)$ представляют собой координаты центра 
ограничивающей рамки, относительно клетки объекта, а числа $(w, h)$ соответственно являются шириной и высотой рамки. Число  $ a $
представляет собой точность предсказания. Также для каждой из клеток сетки предсказывается вероятность принадлежности к тому 
или иному классу. Если предположить, что общее количество классов равно $ C $, то на выходе из нейронной сети будет 
получаться тензор размером $S \times S \times (5B + C)$.

Так как для одного объекта может быть предсказано больше одной ограничивающей рамки, то для выбора наилучшей используют 
метод Non-Max Suppression (NMS). На рисунке \ref{nms} представлен результат работы NMS. Рассмотрим этот метод подробнее:

\addimg{nms}{0.7}{Пример работы метода NMS}{nms}

\begin{enumerate}
\item выбрать охватывающую рамку, имеющую самую большую точность предсказания;
\item отбросить все рамки, которые имеют площадь перекрытия с выбранной рамкой больше определенной границы.
Для этого используется значение Intersection over Union, которое рассчитывается как отношение площади пересечения 
двух рамок к площади их объединения;
\item перейти к первому шагу, пока есть лишние рамки. 
\end{enumerate}

Также большим преимуществом метода YOLO является тот факт, что авторы предоставляют версии предобученных моделей 
для запуска в разных окружениях. Например, в последней работе авторы представили легковесную модель YOLOv4-tiny, 
которая очень хорошо подходит для запуска на устройствах с ограниченными вычислительными ресурсами, при этом средняя 
точность составляет 42\% \cite{YOLO4_SCALED}.

\subsection{Архитектура процессоров ARM}

\subsubsection{Особенности}
В настоящее время большинство устройств Интернета вещей находятся под управлением именно процессоров архитектуры ARM. 
Прямым конкурентом ARM является не менее известная архитектура процессоров -- x86. Данная архитектура изначально разработана
компанией Intel и большинство процессоров настольных компьютеров и серверов реализуют эту архитектуру. Однако со временем 
повсеместное использование процессоров x86 в перечисленных областях может склониться в сторону ARM, для этого достаточно 
вспомнить, как в 2020 году компания Apple перешла на использование в своих ноутбуках процессоров архитектуры ARM \cite{APPLE_M1}.

Архитектура ARM является типичным представителем архитектуры Reduced Instruction Set Computer (RISC), в то время, как 
x86 относится к архитектуре Complex Instruction Set Computer (CISC). Рассмотрим отличительные особенности архитектуры 
RISC и сравним их с CISC:

\begin{enumerate}
\item меньшее количество инструкций. Так как чаще всего используется только ограниченный набор популярных инструкций, то 
аппаратные блоки для менее популярных инструкций в архитектуре CISC не задействованы и только занимают место на кристалле. 
Недостатком этой особенности может быть увеличение размера программы, но при текущем количестве памяти у компьютеров это 
не является большой проблемой;
\item фиксированный размер и простой формат инструкций. Это приводит к тому, что операция декодирования инструкции занимает 
меньше времени и сама аппаратная структура декодировщика остается простой. В случае CISC операция декодирования будет 
занимать больше времени, даже для самых простых инструкций \cite{DIGITAL_DESIGN};
\item инструкции выполняется за один такт. Современные процессоры часто используют конвейеризацию инструкций и данная 
особенность RISC позволяет повысить производительность конвейеров;  
\item большое количество регистров. Например, процессоры архитектуры ARMv8-a имеют 31 регистр общего назначения \cite{ARM_REF}, 
в то время как x86\_64 предоставляет только 16 регистров общего назначения \cite{INTEL_REF}. В большей степени это связано 
с тем, что операции обработки данных в RISC в качестве операндов могут использовать только регистры, поэтому перед 
обработкой данных их нужно загрузить из памяти, а после сохранить результат, используя соответствующие инструкции.
\end{enumerate}

Стоит также упомянуть, что для архитектуры x86 некоторые описанные выше проблемы CISC со временем начали усугубляться из-за
необходимости сохранять поддержку не самых популярных инструкций, которые были добавлены десятки лет назад. Поэтому начиная
с середины 90-х годов процессоры архитектуры x86 разбивают сложную инструкцию на несколько микроопераций (micro-operations),
а после эти инструкции уже выполняются RISC ядром \cite{DIGITAL_DESIGN}.

Также отличительной особенностью процессоров архитектуры ARM является пониженное энергопотребление, по сравнению с процессорами
на x86 архитектуре. Например, тот же ARM процессор Apple M1 на данный момент обладает одной из наилучших производительностей 
на один ватт \cite{APPLE_M1}. Такое достигается за счет использования гетерогенной архитектуры процессоров big.LITTLE, 
когда используется два типа ядер: высокопроизводительные (big) и энергоэффективные (LITTLE). Высокопроизводительные 
используются для требовательных задач, в то время как энергоэффективные активизируются для менее ресурсоемких задач, 
но при этом обладают пониженным энергопотреблением.

\subsubsection{Наборы команд}
Со временем разброс устройств, для которых применялись процессоры ARM стал увеличиваться и тогда компания решила ввести 
понятие профиля архитектуры, который характеризует класс решаемых задач. На данный момент определены три профиля \cite{ARM_REF}:

\begin{enumerate}
\item профиль приложений (application profile). К данному профилю относятся устройства, требующие высокой производительности,
такие как: персональные компьютеры, планшеты, смартфоны. Процессоры данного профиля имеют поддержку виртуальной памяти за
счет наличия устройства управления памятью (memory managment unit, MMU) \cite{ARM_REF};
\item профиль реального-времени (real-time profile). Процессоры данного профиля применяются для решения задач жесткого 
реального времени, то есть таких задач, где требуется гарантия, что обработка события займет не более определенного 
количества времени. Примером таких систем может служить антиблокировочная система тормозов на автомобилях и самолетах. 
Особенностью данного профиля является отсутствие MMU и наличие тесно связанной памяти (Tightly Coupled Memory, TCM). 
TCM - это область памяти, которая по своей структуре похожа на кэш процессора, но содержимым этой памяти строго управляет 
программист и доступ к этой части памяти занимает одни такт процессора;
\item профиль микроконтроллеров (microcontroller profile). Подобные процессоры обладают пониженным энергопотреблением и
позволяют быстрее обрабатывать внешние прерывания \cite{ARM_REF}. Может использоваться в качестве процессоров для 
микроконтроллеров или как составная часть системы на кристалле, например, может выступать контроллером ввода-вывода.
\end{enumerate}

После выхода архитектуры ARMv8 у процессоров появилось два режима работы (execution state): aarch64 и aarch32.
В режиме работы aarch32 используются 32-битные адреса, есть доступ к 13 регистрам общего назначения размером 32 бита, 
имеется 32 64-битных регистра для SIMD инструкций и в качестве набора инструкций могут использоваться A32 или T32 \cite{ARM_REF}.
В свою очередь в режиме aarch64 используются 64-битные адреса, количество регистров общего назначения увеличилось до 31, 
размерность регистров для SIMD инструкций выросла до 128 бит, а в качестве набора команд может использоваться A64 \cite{ARM_REF}.
Процессоры, начиная с архитектуры ARMv8, могут исполнять программы в двух режимах, тем самым сохраняется обратная совместимость
с ранее разработанным ПО, для которого использовался набор инструкций A32 или T32.

Как ранее упоминалось, процессоры архитектуры ARM поддерживают SIMD (Single Instruction Multiple Data) инструкции, для 
этого используется расширение набора команд -- NEON. Данный тип инструкций позволяет за одну инструкцию процессора 
выполнить несколько операций, например, перемножить между собой векторы чисел, каждый из которых содержит по 4 числа.
Подобные инструкции могут значительно ускорить операции кодирования/декодирования видео, потому что при решении таких 
задач часто оперируют векторами чисел. Этими инструкциями можно воспользоваться напрямую при написании на ассемблере, либо 
использовать интринсики компиляторов на языке С/С++. Также некоторые библиотеки могут использовать при своей работе SIMD 
инструкции, например, такую поддержку имеет популярная библиотека OpenCV, разработанная для решения задач компьютерного 
зрения.

\clearpage
